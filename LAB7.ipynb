{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "    embed-resources: true\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EyihIVw3tR4u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "ha = pd.read_csv(\"https://www.dropbox.com/s/aohbr6yb9ifmc8w/heart_attack.csv?dl=1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "cp_dummies = pd.get_dummies(ha[\"cp\"], prefix=\"cp\")\n",
        "ha = pd.concat([ha.drop(columns=[\"cp\"]), cp_dummies], axis=1)\n",
        "ha[\"asymptomatic\"] = ha[\"cp_0\"]\n",
        "ha[\"typical_angina\"] = ha[\"cp_1\"]\n",
        "ha[\"atypical_angina\"] = ha[\"cp_2\"]\n",
        "ha[\"non-anginal_pain\"] = ha[\"cp_3\"]\n",
        "ha = pd.concat([ha.drop(columns=[\"cp_0\",\"cp_1\",\"cp_2\",\"cp_3\"])])\n",
        "cp_dummies = pd.get_dummies(ha[\"restecg\"], prefix=\"re\")\n",
        "ha = pd.concat([ha.drop(columns=[\"restecg\"]), cp_dummies], axis=1)\n",
        "ha[\"ecg_normal\"] = ha[\"re_0\"]\n",
        "ha[\"ecg_abnormality\"] = ha[\"re_1\"]\n",
        "ha[\"ecg_hypertrophy\"] = ha[\"re_2\"]\n",
        "ha = pd.concat([ha.drop(columns=[\"re_0\",\"re_1\",\"re_2\"])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>trtbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>thalach</th>\n",
              "      <th>output</th>\n",
              "      <th>asymptomatic</th>\n",
              "      <th>typical_angina</th>\n",
              "      <th>atypical_angina</th>\n",
              "      <th>non-anginal_pain</th>\n",
              "      <th>ecg_normal</th>\n",
              "      <th>ecg_abnormality</th>\n",
              "      <th>ecg_hypertrophy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>150</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>187</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>178</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "      <td>192</td>\n",
              "      <td>148</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  trtbps  chol  ...  non-anginal_pain  ecg_normal  ecg_abnormality  ecg_hypertrophy\n",
              "0   63    1     145   233  ...              True        True            False            False\n",
              "1   37    1     130   250  ...             False       False             True            False\n",
              "2   56    1     120   236  ...             False       False             True            False\n",
              "3   57    0     120   354  ...             False       False             True            False\n",
              "4   57    1     140   192  ...             False       False             True            False\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ha.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, ConfusionMatrixDisplay, confusion_matrix, RocCurveDisplay\n",
        "\n",
        "X = ha.drop(\"output\", axis=1)\n",
        "y = ha[\"output\"]\n",
        "\n",
        "num_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), num_cols),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1 Q1: KNN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best KNN Params: {'knn__n_neighbors': 9, 'knn__weights': 'distance'}\n",
            "Best CV ROC AUC: 0.8665967908902692\n",
            "[[18  8]\n",
            " [13 16]]\n"
          ]
        }
      ],
      "source": [
        "knn_pipe = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"knn\", KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "knn_grid = {\n",
        "    \"knn__n_neighbors\": [3,5,7,9,11],\n",
        "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
        "}\n",
        "\n",
        "knn_cv = GridSearchCV(knn_pipe, knn_grid, cv=5, scoring=\"roc_auc\")\n",
        "knn_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best KNN Params:\", knn_cv.best_params_)\n",
        "print(\"Best CV ROC AUC:\", knn_cv.best_score_)\n",
        "\n",
        "knn_best = knn_cv.best_estimator_\n",
        "y_pred_knn = knn_best.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_knn))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1 Q2: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Logistic Regression Params: {'logreg__C': 0.1, 'logreg__penalty': 'l2'}\n",
            "Best CV ROC AUC: 0.8970807453416147\n",
            "[[18  8]\n",
            " [11 18]]\n",
            "Logistic Regression Coefficients:\n",
            "[[-0.23739389 -0.73383244 -0.09770441 -0.12483801  0.79112446  0.52842378 -0.52854617\n",
            "  -0.27401039  0.27388799 -0.18963823  0.18951583 -0.06514235  0.06501996  0.01316148\n",
            "  -0.01328388 -0.0472266   0.0471042   0.03382033 -0.03394272]]\n"
          ]
        }
      ],
      "source": [
        "log_pipe = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"logreg\", LogisticRegression())\n",
        "])\n",
        "\n",
        "log_grid = {\n",
        "    \"logreg__C\": [0.01, 0.1, 1, 5, 10],\n",
        "    \"logreg__penalty\": [\"l2\"]\n",
        "}\n",
        "\n",
        "log_cv = GridSearchCV(log_pipe, log_grid, cv=5, scoring=\"roc_auc\")\n",
        "log_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Logistic Regression Params:\", log_cv.best_params_)\n",
        "print(\"Best CV ROC AUC:\", log_cv.best_score_)\n",
        "\n",
        "log_best = log_cv.best_estimator_\n",
        "y_pred_log = log_best.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_log))\n",
        "\n",
        "final_lr = log_cv.best_estimator_.named_steps[\"logreg\"]\n",
        "print(\"Logistic Regression Coefficients:\")\n",
        "print(final_lr.coef_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1 Q3: Descision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Decision Tree Params: {'tree__max_depth': 3, 'tree__min_samples_leaf': 1, 'tree__min_samples_split': 2}\n",
            "Best CV ROC AUC: 0.8276695134575569\n",
            "[[17  9]\n",
            " [11 18]]\n"
          ]
        }
      ],
      "source": [
        "tree_pipe = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"tree\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "tree_grid = {\n",
        "    \"tree__max_depth\": [3,5,7,10, None],\n",
        "    \"tree__min_samples_split\": [2,5,10],\n",
        "    \"tree__min_samples_leaf\": [1,3,5]\n",
        "}\n",
        "\n",
        "tree_cv = GridSearchCV(tree_pipe, tree_grid, cv=5, scoring=\"roc_auc\")\n",
        "tree_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Decision Tree Params:\", tree_cv.best_params_)\n",
        "print(\"Best CV ROC AUC:\", tree_cv.best_score_)\n",
        "\n",
        "tree_best = tree_cv.best_estimator_\n",
        "y_pred_tree = tree_best.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred_tree))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1 Q4: Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>asymptomatic_False</td>\n",
              "      <td>0.568116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thalach</td>\n",
              "      <td>0.210348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sex</td>\n",
              "      <td>0.100022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>trtbps</td>\n",
              "      <td>0.060913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>age</td>\n",
              "      <td>0.060602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chol</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>asymptomatic_True</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>typical_angina_False</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>typical_angina_True</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>atypical_angina_False</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 feature  importance\n",
              "5     asymptomatic_False    0.568116\n",
              "4                thalach    0.210348\n",
              "1                    sex    0.100022\n",
              "2                 trtbps    0.060913\n",
              "0                    age    0.060602\n",
              "3                   chol    0.000000\n",
              "6      asymptomatic_True    0.000000\n",
              "7   typical_angina_False    0.000000\n",
              "8    typical_angina_True    0.000000\n",
              "9  atypical_angina_False    0.000000"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ohe = tree_best.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
        "cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
        "feature_names = np.concatenate([num_cols, cat_feature_names])\n",
        "\n",
        "importances = tree_best.named_steps[\"tree\"].feature_importances_\n",
        "importance_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "importance_df.sort_values(\"importance\", ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "asymptomatic_False,thalach,sex,trtbps, and age were the 5 most important variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1 Q5: ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN\n",
            "TPR (Sensitivity): 0.7673563218390804\n",
            "Precision: 0.7936904761904762\n",
            "TNR (Specificity): 0.7630769230769231\n",
            "\n",
            "\n",
            "LOGISTIC REGRESSION\n",
            "TPR (Sensitivity): 0.7949425287356322\n",
            "Precision: 0.7818471351881663\n",
            "TNR (Specificity): 0.7301538461538462\n",
            "\n",
            "\n",
            "DECISION TREE\n",
            "TPR (Sensitivity): 0.7397701149425286\n",
            "Precision: 0.7957657923175165\n",
            "TNR (Specificity): 0.7787692307692308\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tpr_scorer = make_scorer(recall_score, pos_label=1)\n",
        "\n",
        "precision_scorer = make_scorer(precision_score, pos_label=1)\n",
        "\n",
        "tnr_scorer = make_scorer(recall_score, pos_label=0)\n",
        "\n",
        "knn_tpr = cross_val_score(knn_best, X, y, cv=5, scoring=tpr_scorer).mean()\n",
        "knn_precision = cross_val_score(knn_best, X, y, cv=5, scoring=precision_scorer).mean()\n",
        "knn_tnr = cross_val_score(knn_best, X, y, cv=5, scoring=tnr_scorer).mean()\n",
        "\n",
        "print(\"KNN\")\n",
        "print(\"TPR (Sensitivity):\", knn_tpr)\n",
        "print(\"Precision:\", knn_precision)\n",
        "print(\"TNR (Specificity):\", knn_tnr)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "log_tpr = cross_val_score(log_best, X, y, cv=5, scoring=tpr_scorer).mean()\n",
        "log_precision = cross_val_score(log_best, X, y, cv=5, scoring=precision_scorer).mean()\n",
        "log_tnr = cross_val_score(log_best, X, y, cv=5, scoring=tnr_scorer).mean()\n",
        "\n",
        "print(\"LOGISTIC REGRESSION\")\n",
        "print(\"TPR (Sensitivity):\", log_tpr)\n",
        "print(\"Precision:\", log_precision)\n",
        "print(\"TNR (Specificity):\", log_tnr)\n",
        "print(\"\\n\")\n",
        "\n",
        "tree_tpr = cross_val_score(tree_best, X, y, cv=5, scoring=tpr_scorer).mean()\n",
        "tree_precision = cross_val_score(tree_best, X, y, cv=5, scoring=precision_scorer).mean()\n",
        "tree_tnr = cross_val_score(tree_best, X, y, cv=5, scoring=tnr_scorer).mean()\n",
        "\n",
        "print(\"DECISION TREE\")\n",
        "print(\"TPR (Sensitivity):\", tree_tpr)\n",
        "print(\"Precision:\", tree_precision)\n",
        "print(\"TNR (Specificity):\", tree_tnr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3 Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. In this case I would focus on True Positive Rate (Sensitivity / Recall). We mainly want to avoid false negatives—patients who truly have high heart attack risk but are predicted as “no heart attack.” A high sensitivity means the model is catching most of the patients who are actually positive, even if that means we raise more false alarms.\n",
        "\n",
        "Based on my cross-validated results, the Logistic Regression model has the highest sensitivity, with a TPR of about 0.795, compared to about 0.767 for KNN and 0.740 for the Decision Tree. I would recommend Logistic Regression.\n",
        "\n",
        "From the cross-validation, I would expect a sensitivity of around 0.79 on new patients, with a ROC AUC of about 0.90 (0.897) giving strong overall discrimination between high-risk and low-risk patients.\n",
        "\n",
        "\n",
        "2. Here I would focus on Precision (Positive Predictive Value). When we predict that someone is “high risk,” we want that prediction to be correct as often as possible, because each “high-risk” label uses up a hospital bed. High precision means fewer false positives, so we don’t waste limited resources on people who are actually low risk.\n",
        "\n",
        "Looking at the cross-validated metrics, the Decision Tree model has the highest precision at about 0.796, slightly higher than KNN at about 0.794 and Logistic Regression at about 0.782. I would recommend the Decision Tree model, since it gives the most reliable positive predictions.\n",
        "\n",
        "I would expect a precision of around 0.80 for future patients using the Decision Tree, meaning that roughly 80% of patients the model marks as high risk would truly be high risk.\n",
        "\n",
        "3. For this goal, the main priority is interpretability, not just predictive performance. Instead of focusing on sensitivity or precision, I would focus on models where the relationship between predictors and risk is easy to explain, especially the sign and size of coefficients.\n",
        "\n",
        "I would recommend the Logistic Regression model here. Its coefficients tell us how each variable relates to the odds of a heart attack (positive coefficients increase risk, negative coefficients decrease risk). Combined with the importance results from the Decision Tree, we see that features like asymptomatic chest pain status, maximum heart rate achieved (thalach), sex, resting blood pressure (trtbps), and age are especially important in predicting heart attack risk. \n",
        "\n",
        "Even though explanation is the main goal, it is good that the Logistic Regression still performs well, with a cross-validated ROC AUC of about 0.897. This means the model not only helps us understand root causes, but also separates higher-risk and lower-risk patients quite effectively.\n",
        "\n",
        "4. In this setting, the model is being used as a benchmark to compare against human diagnoses. I would want a model with strong overall discriminative ability and fairly balanced performance, so the main metric I would use is ROC AUC, which summarizes how well the model ranks patients from low to high risk across all possible thresholds. I might also look at overall accuracy or both sensitivity and specificity together, but ROC AUC is the clean summary.\n",
        "\n",
        "Since Logistic Regression has the highest cross-validated ROC AUC (about 0.897), better than KNN (~0.867) and the Decision Tree (~0.828), it is the best candidate to serve as the “gold standard” reference model for comparing to new doctors. \n",
        "\n",
        "I would expect the Logistic Regression model to maintain a ROC AUC of around 0.90 on future patients. That means, in random pairs of patients where one has a heart attack and one does not, the model should correctly rank the higher-risk patient above the lower-risk patient about 90% of the time. This makes it a strong and consistent benchmark for evaluating how well the new doctors’ diagnoses line up with a well-performing algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "ha_validation = pd.read_csv(\"https://www.dropbox.com/s/jkwqdiyx6o6oad0/heart_attack_validation.csv?dl=1\")\n",
        "cp_dummies = pd.get_dummies(ha_validation[\"cp\"], prefix=\"cp\")\n",
        "ha_validation = pd.concat([ha_validation.drop(columns=[\"cp\"]), cp_dummies], axis=1)\n",
        "ha_validation[\"asymptomatic\"] = ha_validation[\"cp_0\"]\n",
        "ha_validation[\"typical_angina\"] = ha_validation[\"cp_1\"]\n",
        "ha_validation[\"atypical_angina\"] = ha_validation[\"cp_2\"]\n",
        "ha_validation[\"non-anginal_pain\"] = ha_validation[\"cp_3\"]\n",
        "ha_validation = pd.concat([ha_validation.drop(columns=[\"cp_0\",\"cp_1\",\"cp_2\",\"cp_3\"])])\n",
        "restecg_dummies = pd.get_dummies(ha_validation[\"restecg\"], prefix=\"re\")\n",
        "ha_validation = pd.concat([ha_validation.drop(columns=[\"restecg\"]), restecg_dummies], axis=1)\n",
        "ha_validation[\"ecg_normal\"] = ha_validation[\"re_0\"]\n",
        "ha_validation[\"ecg_abnormality\"] = ha_validation[\"re_1\"]\n",
        "ha_validation[\"ecg_hypertrophy\"] = 0\n",
        "ha_validation = pd.concat([ha_validation.drop(columns=[\"re_0\",\"re_1\"])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN - Validation\n",
            "Confusion Matrix:\n",
            "[[ 9  2]\n",
            " [ 8 11]]\n",
            "ROC AUC: 0.8421052631578947\n",
            "Precision: 0.8461538461538461\n",
            "Recall: 0.5789473684210527\n",
            "\n",
            "\n",
            "Logistic Regression - Validation\n",
            "Confusion Matrix:\n",
            "[[ 8  3]\n",
            " [ 5 14]]\n",
            "ROC AUC: 0.8708133971291865\n",
            "Precision: 0.8235294117647058\n",
            "Recall: 0.7368421052631579\n",
            "\n",
            "\n",
            "Decision Tree - Validation\n",
            "Confusion Matrix:\n",
            "[[ 8  3]\n",
            " [ 6 13]]\n",
            "ROC AUC: 0.7583732057416267\n",
            "Precision: 0.8125\n",
            "Recall: 0.6842105263157895\n"
          ]
        }
      ],
      "source": [
        "X_val = ha_validation.drop(\"output\", axis=1)\n",
        "y_val = ha_validation[\"output\"]\n",
        "\n",
        "y_pred_knn_val = knn_best.predict(X_val)\n",
        "y_proba_knn_val = knn_best.predict_proba(X_val)[:, 1]\n",
        "\n",
        "cm_knn_val = confusion_matrix(y_val, y_pred_knn_val)\n",
        "knn_val_auc = roc_auc_score(y_val, y_proba_knn_val)\n",
        "knn_val_precision = precision_score(y_val, y_pred_knn_val, pos_label=1)\n",
        "knn_val_recall = recall_score(y_val, y_pred_knn_val, pos_label=1)\n",
        "\n",
        "print(\"KNN - Validation\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_knn_val)\n",
        "print(\"ROC AUC:\", knn_val_auc)\n",
        "print(\"Precision:\", knn_val_precision)\n",
        "print(\"Recall:\", knn_val_recall)\n",
        "print(\"\\n\")\n",
        "\n",
        "y_pred_log_val = log_best.predict(X_val)\n",
        "y_proba_log_val = log_best.predict_proba(X_val)[:, 1]\n",
        "\n",
        "cm_log_val = confusion_matrix(y_val, y_pred_log_val)\n",
        "log_val_auc = roc_auc_score(y_val, y_proba_log_val)\n",
        "log_val_precision = precision_score(y_val, y_pred_log_val, pos_label=1)\n",
        "log_val_recall = recall_score(y_val, y_pred_log_val, pos_label=1)\n",
        "\n",
        "print(\"Logistic Regression - Validation\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_log_val)\n",
        "print(\"ROC AUC:\", log_val_auc)\n",
        "print(\"Precision:\", log_val_precision)\n",
        "print(\"Recall:\", log_val_recall)\n",
        "print(\"\\n\")\n",
        "\n",
        "y_pred_tree_val = tree_best.predict(X_val)\n",
        "y_proba_tree_val = tree_best.predict_proba(X_val)[:, 1]\n",
        "\n",
        "cm_tree_val = confusion_matrix(y_val, y_pred_tree_val)\n",
        "tree_val_auc = roc_auc_score(y_val, y_proba_tree_val)\n",
        "tree_val_precision = precision_score(y_val, y_pred_tree_val, pos_label=1)\n",
        "tree_val_recall = recall_score(y_val, y_pred_tree_val, pos_label=1)\n",
        "\n",
        "print(\"Decision Tree - Validation\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_tree_val)\n",
        "print(\"ROC AUC:\", tree_val_auc)\n",
        "print(\"Precision:\", tree_val_precision)\n",
        "print(\"Recall:\", tree_val_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 4: Comparison\n",
        "\n",
        "Overall, the cross-validated metrics did a pretty good job of predicting how the models would perform on the separate validation set. For all three models, the ROC AUC on the validation data was a little lower than the cross-validated ROC AUC, but the ordering stayed the same: Logistic Regression was best, then KNN, then the Decision Tree. For example, Logistic Regression went from a cross-validated ROC AUC of about 0.90 to about 0.87 on the validation set, while KNN went from about 0.87 to 0.84, and the Decision Tree dropped from about 0.83 to about 0.76.\n",
        "\n",
        "Looking at precision and recall, the patterns are similar. Precision on the validation set is slightly higher than the cross-validated values for all three models, while recall is slightly lower. For KNN, recall dropped more noticeably (from about 0.77 in cross-validation to about 0.58 on validation), which suggests KNN was a bit optimistic about how many true positives it could catch. Logistic Regression and the Decision Tree had smaller changes in recall (both within about 0.06 of their cross-validated values) and similar or slightly better precision. Overall, the differences are within a reasonable range given the small sample size of the validation set, so I’d say our cross-validated measures of model success were approximately correct, especially for Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Cohen’s Kappa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN Cohen’s Kappa: 0.2416283650689428\n",
            "Logistic Regression Cohen’s Kappa: 0.31114040870138426\n",
            "Decision Tree Cohen’s Kappa: 0.27344782034346105\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "y_pred_knn = knn_best.predict(X_test)       \n",
        "y_true      = y_test                        \n",
        "\n",
        "kappa_knn = cohen_kappa_score(y_true, y_pred_knn)\n",
        "print(\"KNN Cohen’s Kappa:\", kappa_knn)\n",
        "\n",
        "y_pred_log = log_best.predict(X_test)\n",
        "kappa_log  = cohen_kappa_score(y_true, y_pred_log)\n",
        "print(\"Logistic Regression Cohen’s Kappa:\", kappa_log)\n",
        "\n",
        "y_pred_tree = tree_best.predict(X_test)\n",
        "kappa_tree  = cohen_kappa_score(y_true, y_pred_tree)\n",
        "print(\"Decision Tree Cohen’s Kappa:\",  kappa_tree)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression has the highest Cohen’s Kappa, followed by the Decision Tree, then KNN. This matches the idea from earlier parts that Logistic Regression is the strongest overall model. It also shows that all three models are doing better than chance, which makes sense given that this is a noisy medical outcome and the dataset is not huge.\n",
        "\n",
        "Cohen’s Kappa is especially useful in scenarios where class imbalance or chance agreement can make accuracy look deceptively good. For example, if almost everyone were “no heart attack,” a model that predicts “no heart attack” for everyone would have high accuracy but a Kappa close to 0. In a hospital setting, Kappa is helpful when we care about overall agreement with the true labels (or with a human expert) and want to discount “easy” agreement that comes from the dominant class.\n",
        "\n",
        "Judging the models by Cohen’s Kappa does not really change my main conclusions from earlier parts. Logistic Regression was already the best model by ROC AUC, and it is still best by Kappa. The only small change is that KNN and the Decision Tree swap order (KNN had slightly better ROC AUC than the Tree, but a lower Kappa), which just reflects that Kappa and ROC AUC emphasize different things. ROC AUC focuses on ranking patients by risk across thresholds, while Kappa focuses on the actual 0/1 classification at the chosen cutoff and adjusts for chance. It makes sense that they give slightly different rankings for KNN vs. the Tree, but the big picture, Logistic Regression, the strongest overall model stays the same."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
