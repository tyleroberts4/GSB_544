{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "    embed-resources: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrJX4FDa8oA8"
      },
      "source": [
        "# XML, HTML, and Web Scraping\n",
        "\n",
        "JSON and XML are two different ways to represent hierarchical data. Which one is better? There are lots of articles online which discuss similarities and differences between JSON and XML and their advantages and disadvantages. Both formats are still in current usage, so it is good to be familiar with both. However, JSON is more common, so we'll focus on working with JSON representations of hierarchical data.\n",
        "\n",
        "The reading covered an example of using Beautiful Soup to parse XML. Rather than doing another example XML now, we'll skip straight to scraping HTML from a webpage. Both HTML and XML can be parsed in a similar way with Beautiful Soup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XZhT8jhbuZSg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApqnMQ4iV4qu"
      },
      "source": [
        "## Scraping an HTML table with Beautiful Soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SD7XOs_So3G"
      },
      "source": [
        "Open the URL https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population and scroll down until you see a table of the cities in the U.S. with population over 100,000 (as of Jul 1, 2022). We'll use Beautiful Soup to scrape information from this table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRmnzgaQS_T0"
      },
      "source": [
        "Read in the HTML from the URL using the `requests` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xvYzbSospYVu"
      },
      "outputs": [],
      "source": [
        "URL = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "response = requests.get(URL, headers=HEADERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ1Swg6B82_J"
      },
      "source": [
        "# Use Beautiful Soup to parse this string into a tree called `soup`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e0jpmfwtpaEB"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# If your HTML is in `response.text` from requests:\n",
        "soup = BeautifulSoup(response.text, \"lxml\")  # or \"html.parser\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFxGW_KIDjnx"
      },
      "source": [
        "To find an HTML tag corresponding to a specific element on a webpage, right-click on it and choose \"Inspect element\". Go to the cities table Wikipedia page and do this now.\n",
        "\n",
        "You should find that the cities table on the Wikipedia page corresponds to an element that looks like\n",
        "\n",
        "```\n",
        "<table class=\"______\" style=\"_______\">\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR50aTBZEwov"
      },
      "source": [
        "There are many `<table>` tags on the page.  One option you have is to find them all, and then manually figure out which one in this list is the table you care about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4691d-EGEwc0",
        "outputId": "39821c63-0a6e-4d20-9d16-d1b0b83806d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_tables = len(soup.find_all(\"table\"))\n",
        "all_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1xslM2yE1GI"
      },
      "source": [
        "\n",
        "\n",
        "The other option is to try to use attributes like `class=` and/or `style=` to narrow down the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Q0sa46DvTZ",
        "outputId": "efae0423-ebec-4bfe-fcb9-5cd046e7419e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Option 2: match class + style (stricter; may miss if style changes)\n",
        "tables = soup.find_all(\"table\",\n",
        "    attrs={\n",
        "        \"class\": [\"wikitable\", \"sortable\"],\n",
        "        \"style\": \"text-align:right\"\n",
        "    }\n",
        ")\n",
        "len(tables)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndnRSJJiFFby"
      },
      "source": [
        "At this point, you can manually inspect the tables on the webpage to find that the one we want is the first one (see `[0]` below). We'll store this as `table`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sRBSqVGlYhuT"
      },
      "outputs": [],
      "source": [
        "table = tables[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4AWo3QoYqNY"
      },
      "source": [
        "**Now you will write code to scrape the information in `table` to create a Pandas data frame with one row for each city and columns for: city, state, population (2022 estimate), and 2020 land area (sq mi).** Refer to the Notes/suggestions below as you write your code. A few Hints are provided further down, but try coding first before looking at the hints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfRx2_XlDUqD"
      },
      "source": [
        "Notes/suggestions:\n",
        "\n",
        "- Use as a guide the code from the reading that produced the data frame of Statistics faculty\n",
        "- Inspect the page source as you write your code\n",
        "- You will need to write a loop to get the information for all cities, but you might want to try just scraping the info for New York first\n",
        "- You will need to pull the text from the tag. If `.text` returns text with \"\\n\" at the end, try `.get_text(strip = True)` instead of `.text`\n",
        "- Don't forget to convert to a Pandas Data Frame; it should have 333 rows and 4 columns\n",
        "- The goal of this exercise is just to create the Data Frame. If you were going to use it --- e.g., what is the population density for all cities in CA? --- then you would need to clean the data first (to clean strings and convert to quantitative). (You can use Beautiful Soup to do some of the cleaning for you, but that goes beyond our scope.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msKiUcOZpSX7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE. ADD AS MANY CELLS AS NEEDED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M23w-PPtUyw",
        "outputId": "ca5818b0-2d61-4d66-9e30-d6850d68c4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['City', 'ST', '2024 estimate', '2020 census', 'Change', '2020 land area', '2020 density', 'Location']\n",
            "(346, 4)\n",
            "           city state population (2022 estimate) 2020 land area (sq mi)\n",
            "0      New York    NY                  8,478,072                  300.5\n",
            "1   Los Angeles    CA                  3,878,704                  469.5\n",
            "2       Chicago    IL                  2,721,308                  227.7\n",
            "3       Houston    TX                  2,390,125                  640.4\n",
            "4       Phoenix    AZ                  1,673,164                  518.0\n",
            "5  Philadelphia    PA                  1,573,916                  134.4\n",
            "6   San Antonio    TX                  1,526,656                  498.8\n",
            "7     San Diego    CA                  1,404,452                  325.9\n",
            "8        Dallas    TX                  1,326,087                  339.6\n",
            "9  Jacksonville    FL                  1,009,833                  747.3\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "# --- fetch & parse ---\n",
        "resp = requests.get(URL, headers=HEADERS)\n",
        "resp.raise_for_status()\n",
        "soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "table = soup.select_one(\"table.wikitable.sortable\")\n",
        "if table is None:\n",
        "    raise RuntimeError(\"Could not find the main population table.\")\n",
        "\n",
        "# --- header normalization helpers ---\n",
        "def norm(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"\\[[^\\]]*\\]\", \"\", s)          # strip [a] style notes\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "thead = table.find(\"thead\")\n",
        "header_cells = (thead.find_all(\"th\") if thead else None) or table.find(\"tr\").find_all([\"th\",\"td\"])\n",
        "headers_raw = [th.get_text(\" \", strip=True) for th in header_cells]\n",
        "headers = [norm(h) for h in headers_raw]\n",
        "\n",
        "# --- find columns robustly ---\n",
        "def find_state_idx(headers):\n",
        "    for i, h in enumerate(headers):\n",
        "        if h in {\"state\", \"st\"} or (\"state\" in h and len(h) <= 10):\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def find_city_idx(headers):\n",
        "    for i, h in enumerate(headers):\n",
        "        if \"city\" in h:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def find_land2020_idx(headers):\n",
        "    # Accept \"2020 land area\" with/without \"(sq mi)\"\n",
        "    for i, h in enumerate(headers):\n",
        "        if \"2020\" in h and \"land area\" in h:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def find_pop_est_idx(headers, prefer_years=(2024, 2023, 2022)):\n",
        "    # Prefer newest estimate if multiple are present; fall back to any \"estimate\"\n",
        "    for yr in prefer_years:\n",
        "        for i, h in enumerate(headers):\n",
        "            if str(yr) in h and \"estimate\" in h:\n",
        "                return i\n",
        "    # fallback: first header containing \"estimate\"\n",
        "    for i, h in enumerate(headers):\n",
        "        if \"estimate\" in h:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "idx_city  = find_city_idx(headers)\n",
        "idx_state = find_state_idx(headers)\n",
        "idx_land  = find_land2020_idx(headers)\n",
        "idx_pop   = find_pop_est_idx(headers)\n",
        "\n",
        "missing = [name for name, idx in {\n",
        "    \"city\": idx_city, \"state\": idx_state, \"land\": idx_land, \"pop_est\": idx_pop\n",
        "}.items() if idx is None]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Could not map required columns. Missing: {missing}\\nHeaders seen: {headers}\")\n",
        "\n",
        "# --- extract rows ---\n",
        "rows = []\n",
        "tbody = table.find(\"tbody\") or table\n",
        "for tr in tbody.find_all(\"tr\"):\n",
        "    tds = tr.find_all(\"td\")\n",
        "    if len(tds) <= max(idx_city, idx_state, idx_land, idx_pop):\n",
        "        continue\n",
        "\n",
        "    # drop inline citations\n",
        "    for td in tds:\n",
        "        for sup in td.select(\"sup\"):\n",
        "            sup.decompose()\n",
        "\n",
        "    def txt(i): return tds[i].get_text(\" \", strip=True)\n",
        "\n",
        "    rows.append({\n",
        "        \"city\": txt(idx_city),\n",
        "        \"state\": txt(idx_state),                                  # column is \"st\" on the page; we normalize name here\n",
        "        \"population (2022 estimate)\": txt(idx_pop),               # we keep your required label even if source is 2024\n",
        "        \"2020 land area (sq mi)\": txt(idx_land),                  # unit label added for clarity\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    \"city\", \"state\", \"population (2022 estimate)\", \"2020 land area (sq mi)\"\n",
        "])\n",
        "\n",
        "print(headers_raw)         # (optional) see exact headers found\n",
        "print(df.shape)\n",
        "print(df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s3tH82XZ1X0"
      },
      "source": [
        "Hints:\n",
        "\n",
        "- Each city is a row in the table; find all the `<tr>` tags to find all the cities\n",
        "- Look for the `<td>` tag to see table entries within a row\n",
        "- The rank column is represented by `<th>` tags, rather than `<td>` tags. So within a row, the first (that is, `[0]`) `<td>` tag corresponds to the city name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctj79YpgX6hw"
      },
      "source": [
        "## Scraping information that is NOT in a `<table>` with Beautiful Soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK6rJQbuuWwF"
      },
      "source": [
        "The Cal Poly course catalog http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory contains a list of courses offered by the Statistics department. **You will scrape this website to obtain a Pandas data frame with one row for each DATA or STAT course and two columns: course name and number (e.g, DATA 301. Introduction to Data Science) and term typically offered (e.g., Term Typically Offered: F, W, SP).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbLLrwxs0eWd"
      },
      "source": [
        "Note: Pandas `read_html` is not help here since the courses are not stored in a `<table>.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIRewkca0jhz",
        "outputId": "3fb320b0-8b95-4249-c139-7c7c0f43e59d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[                                        Program name   Program type\n",
              " 0                              Actuarial Preparation          Minor\n",
              " 1  Cross Disciplinary Studies Minor in Bioinforma...          Minor\n",
              " 2   Cross Disciplinary Studies Minor in Data Science          Minor\n",
              " 3                                         Statistics  BS, MS, Minor]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_html(\"http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvSrhxS4Se7a"
      },
      "source": [
        "\n",
        "Notes/suggestions:\n",
        "\n",
        "\n",
        "- Inspect the page source as you write your code\n",
        "- The courses are not stored in a `<table>`. How are they stored?\n",
        "- You will need to write a loop to get the information for all courses, but you might want to try just scraping the info for DATA 100 first\n",
        "- What kind of tag is the course name stored in? What is the `class` of the tag?\n",
        "- What kind of tag is the quarter(s) the course is offered stored in? What is the `class` of the tag? Is this the only tag of this type with the class? How will you get the one you want?\n",
        "- You don't have to remove the number of units (e.g., 4 units) from the course name and number, but you can try it if you want\n",
        "- You will need to pull the text from the tag. If `.text` returns text with \"\\n\" at the end, try `get_text(strip = True)` instead of `text`\n",
        "- Don't forget to convert to a Pandas Data Frame; it should have 74 rows and 2 columns\n",
        "- The goal of this exercise is just to create the Data Frame. If you were going to use it then you might need to clean the data first. (You can use Beautiful Soup to do some of the cleaning for you, but that goes beyond our scope.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbW6xon4vICB"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE. ADD AS MANY CELLS AS NEEDED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW2sWIGavIFc",
        "outputId": "f4bb7bad-d482-4553-ca11-a48920976a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(74, 2)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "# 1) Fetch & parse\n",
        "resp = requests.get(URL, headers=HEADERS)\n",
        "resp.raise_for_status()\n",
        "soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "# 2) Each course lives in a block like: <div class=\"courseblock\"> ... </div>\n",
        "course_blocks = soup.select(\"div.courseblock\")\n",
        "\n",
        "rows = []\n",
        "for block in course_blocks:\n",
        "    # Course name/number line is in: <p class=\"courseblocktitle\">DATA 100. Intro ... (4 units)</p>\n",
        "    title_tag = block.select_one(\"p.courseblocktitle\")\n",
        "    if not title_tag:\n",
        "        continue\n",
        "    title_text = title_tag.get_text(\" \", strip=True)\n",
        "\n",
        "    # Keep only DATA or STAT courses\n",
        "    if not re.match(r\"^(DATA|STAT)\\s\", title_text):\n",
        "        continue\n",
        "\n",
        "    # Find the line that holds \"Term Typically Offered: ...\"\n",
        "    term_text = None\n",
        "    for p in block.select(\"p.courseblockdetail\"):\n",
        "        t = p.get_text(\" \", strip=True)\n",
        "        if t.startswith(\"Term Typically Offered\"):\n",
        "            term_text = t\n",
        "            break\n",
        "\n",
        "    # If the term line is missing, store a placeholder so row count stays consistent\n",
        "    if term_text is None:\n",
        "        term_text = \"Term Typically Offered: â€”\"\n",
        "\n",
        "    rows.append({\n",
        "        \"course\": title_text,                 # e.g., \"DATA 301. Introduction to Data Science (4 units)\"\n",
        "        \"term typically offered\": term_text   # e.g., \"Term Typically Offered: F, W, SP\"\n",
        "    })\n",
        "\n",
        "# 3) Build the DataFrame\n",
        "df = pd.DataFrame(rows, columns=[\"course\", \"term typically offered\"])\n",
        "\n",
        "print(df.shape)    # should be (74, 2) per the prompt (may change if catalog updates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e8M_OsaHJz"
      },
      "source": [
        "Hints:\n",
        "\n",
        "- Each course is represented by a `<div>` with `class=courseblock`, so you can find all the courses with `soup.find_all(\"div\", {\"class\": \"courseblock\"})`\n",
        "- The course name is in a `<p>` tag with `class=courseblocktitle`, inside a `<strong>` tag. (Though I don't think we need to find the strong tag here.)\n",
        "- The term typically offered is in `<p>` tag with `class=noindent`. However, there are several tags with this class; term typically offered is the first one.\n",
        "- If you want to use Beautiful Soup to remove the course units (e.g., 4 units), find the `<span>` tag within the course name tag and `.extract()` this span tag"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
